# NATIONAL AI OVERSIGHT AND COORDINATION ACT
## Model Legislation for the Human Standard Framework

---

## TITLE I: SHORT TITLE AND FINDINGS

### Section 101. Short Title
This Act may be cited as the "National AI Oversight and Coordination Act" or the "NAICO Act."

### Section 102. Findings
Congress finds the following:

(1) Artificial intelligence systems increasingly make decisions affecting the lives, rights, and welfare of Americans.

(2) Current governance structures are inadequate to address the speed and scope of AI deployment.

(3) A coordinated national approach is necessary to ensure AI serves human welfare.

(4) Transparency in AI systems is essential for democratic accountability.

(5) The United States must lead in responsible AI governance to shape global norms.

(6) Open source AI governance systems can prevent corruption and ensure public trust.

---

## TITLE II: ESTABLISHMENT OF NAICO

### Section 201. National AI Coordination Office

**(a) ESTABLISHMENT.** There is established in the Executive Office of the President the National AI Coordination Office (NAICO).

**(b) DIRECTOR.** NAICO shall be headed by a Director appointed by the President and confirmed by the Senate for a 5-year term.

**(c) QUALIFICATIONS.** The Director shall have:
   (1) Significant expertise in artificial intelligence or related fields;
   (2) Experience in public policy or administration;
   (3) Demonstrated commitment to democratic governance and human rights.

**(d) INDEPENDENCE.** The Director may only be removed for cause, and shall not be subject to political direction on technical determinations.

### Section 202. Mission and Functions

**(a) MISSION.** NAICO's mission is to ensure that artificial intelligence deployed in the United States serves human welfare, protects individual rights, and maintains democratic accountability.

**(b) FUNCTIONS.** NAICO shall:
   (1) Coordinate federal AI policy across agencies;
   (2) Establish standards for AI transparency, safety, and fairness;
   (3) Maintain a public registry of high-impact AI systems;
   (4) Investigate complaints regarding AI harms;
   (5) Conduct research on AI governance;
   (6) Advise Congress and the President on AI policy;
   (7) Represent the United States in international AI governance forums.

### Section 203. Organization

**(a) DIVISIONS.** NAICO shall include the following divisions:
   (1) Standards and Certification Division;
   (2) Enforcement and Investigation Division;
   (3) Research and Assessment Division;
   (4) Public Engagement and Education Division;
   (5) International Coordination Division.

**(b) STAFFING.** NAICO is authorized to employ up to 500 full-time personnel initially, expanding as needed.

**(c) BUDGET.** There are authorized to be appropriated:
   (1) $100 million for Year 1;
   (2) $200 million for Year 2;
   (3) $500 million for Year 3 and each subsequent year.

### Section 204. Advisory Bodies

**(a) AI GOVERNANCE COUNCIL.** There is established an AI Governance Council consisting of:
   (1) Representatives from relevant federal agencies;
   (2) Representatives from state governments;
   (3) Industry representatives;
   (4) Academic experts;
   (5) Civil society representatives;
   (6) Representatives of affected communities.

**(b) CITIZEN REVIEW BOARD.** There is established a Citizen Review Board consisting of:
   (1) Twenty members selected by lottery from eligible citizens;
   (2) Serving staggered 2-year terms;
   (3) Providing input on standards and policies from a public perspective.

---

## TITLE III: AI CLASSIFICATION AND REGISTRATION

### Section 301. Risk-Based Classification

**(a) CATEGORIES.** AI systems shall be classified into the following risk categories:

**(1) MINIMAL RISK:** AI systems with negligible impact on individuals, including:
   - Spam filters;
   - Recommendation systems for entertainment;
   - Video game AI;
   - General-purpose productivity tools.

**(2) LIMITED RISK:** AI systems with modest but manageable impact, including:
   - Customer service chatbots;
   - Content moderation tools;
   - Search ranking algorithms;
   - Basic data analytics.

**(3) SIGNIFICANT RISK:** AI systems materially affecting important interests, including:
   - Employment screening and hiring;
   - Credit scoring and lending decisions;
   - Insurance underwriting;
   - Educational assessments;
   - Healthcare diagnostics (advisory);
   - Content algorithms affecting public discourse.

**(4) HIGH RISK:** AI systems affecting fundamental rights or safety, including:
   - Criminal justice (sentencing, parole, policing);
   - Immigration decisions;
   - Child welfare determinations;
   - Healthcare treatment decisions;
   - Critical infrastructure control;
   - Autonomous vehicles;
   - Weapons systems.

**(5) PROHIBITED:** AI systems that are not permitted, including:
   - Social scoring systems;
   - Subliminal manipulation;
   - Exploiting vulnerabilities of specific groups;
   - Lethal autonomous weapons without human control;
   - Mass surveillance for social control.

### Section 302. Registration Requirements

**(a) SIGNIFICANT RISK SYSTEMS.** Deployers of significant risk AI systems shall register with NAICO, providing:
   (1) System description and purpose;
   (2) Training data sources;
   (3) Accuracy and error metrics;
   (4) Bias auditing results;
   (5) Planned monitoring procedures.

**(b) HIGH RISK SYSTEMS.** Deployers of high risk AI systems shall additionally:
   (1) Obtain pre-deployment certification;
   (2) Conduct independent third-party audits;
   (3) Implement continuous monitoring;
   (4) Provide incident reporting;
   (5) Maintain human oversight protocols.

**(c) TIMELINE.**
   (1) Existing systems: 12 months to register after effective date;
   (2) New systems: Pre-deployment registration required.

### Section 303. Public Registry

**(a) ESTABLISHMENT.** NAICO shall maintain a public registry of all registered AI systems.

**(b) CONTENTS.** The registry shall include:
   (1) System name and deployer;
   (2) Risk category and justification;
   (3) General description of function;
   (4) Summary of bias auditing results;
   (5) Incident reports (redacted as necessary);
   (6) Certification status.

**(c) TRADE SECRETS.** Proprietary information may be withheld from public disclosure but must be provided to NAICO.

---

## TITLE IV: TRANSPARENCY AND EXPLAINABILITY

### Section 401. Disclosure Requirements

**(a) NOTICE OF AI USE.** When an AI system significantly affects an individual, that individual must be informed:
   (1) That an AI system was used;
   (2) What decision or action the AI affected;
   (3) How to obtain more information;
   (4) How to appeal or request human review.

**(b) MEANINGFUL EXPLANATION.** Upon request, affected individuals have the right to:
   (1) A plain-language explanation of how the AI reached its conclusion;
   (2) The key factors that influenced the decision;
   (3) What alternative inputs might change the outcome;
   (4) The accuracy and error rates of the system.

### Section 402. Government AI Transparency

**(a) OPEN SOURCE REQUIREMENT.** AI systems used by federal agencies for administrative decisions affecting the public shall be:
   (1) Open source, with publicly available code;
   (2) Auditable by independent researchers;
   (3) Subject to public comment before deployment;
   (4) Documented with training data sources.

**(b) EXCEPTIONS.** The open source requirement does not apply to:
   (1) National security applications (with congressional oversight);
   (2) Active law enforcement investigations;
   (3) Cybersecurity defensive systems.

**(c) TRANSITION.** Existing federal AI systems shall comply within 3 years of enactment.

### Section 403. Algorithmic Impact Assessments

**(a) REQUIREMENT.** Before deploying a high-risk AI system, deployers shall complete an Algorithmic Impact Assessment including:
   (1) Purpose and necessity of the system;
   (2) Potential harms and affected populations;
   (3) Data sources and quality;
   (4) Bias testing methodology and results;
   (5) Mitigation measures for identified risks;
   (6) Monitoring and evaluation plans;
   (7) Human oversight procedures.

**(b) PUBLIC PARTICIPATION.** For government AI systems, draft assessments shall be:
   (1) Published for public comment for 60 days;
   (2) Revised based on public input;
   (3) Finalized with responses to significant comments.

---

## TITLE V: FAIRNESS AND NON-DISCRIMINATION

### Section 501. Prohibited Discrimination

**(a) GENERAL PROHIBITION.** AI systems shall not discriminate on the basis of:
   (1) Race, color, or national origin;
   (2) Sex, gender, or sexual orientation;
   (3) Religion;
   (4) Age;
   (5) Disability;
   (6) Genetic information;
   (7) Any other characteristic protected by federal law.

**(b) DISPARATE IMPACT.** Discrimination includes facially neutral practices that have unjustified disparate impact on protected groups.

### Section 502. Bias Auditing

**(a) REQUIREMENT.** Significant and high-risk AI systems shall undergo bias auditing:
   (1) Before deployment;
   (2) Annually thereafter;
   (3) After significant system changes;
   (4) Upon complaint triggering investigation.

**(b) METHODOLOGY.** Bias audits shall include:
   (1) Analysis of outcomes by protected characteristics;
   (2) Testing with representative samples;
   (3) Evaluation of training data for historical bias;
   (4) Assessment of proxy discrimination.

**(c) AUDITORS.** Bias audits for high-risk systems shall be conducted by:
   (1) NAICO-certified independent auditors; or
   (2) NAICO directly upon request.

### Section 503. Remediation

**(a) CORRECTIVE ACTION.** If bias is identified:
   (1) Deployment shall be halted pending correction;
   (2) Affected individuals shall be notified;
   (3) Remediation plan shall be submitted to NAICO;
   (4) Re-auditing required before resumption.

**(b) ENFORCEMENT.** NAICO may impose:
   (1) Civil penalties up to $10 million per violation;
   (2) Injunctive relief;
   (3) Mandatory system modifications;
   (4) Decertification.

---

## TITLE VI: HUMAN OVERSIGHT AND APPEAL

### Section 601. Human-in-the-Loop Requirements

**(a) HIGH-RISK DECISIONS.** For AI systems affecting fundamental rights, liberties, or safety:
   (1) A human being shall review and approve significant adverse decisions;
   (2) The human reviewer shall have authority to override the AI;
   (3) The human reviewer shall not be penalized for overriding AI recommendations;
   (4) The AI recommendation shall be one input, not determinative.

**(b) TRAINING.** Human reviewers shall receive training on:
   (1) AI system capabilities and limitations;
   (2) Common failure modes;
   (3) How to evaluate AI recommendations;
   (4) When override is appropriate.

### Section 602. Appeal Rights

**(a) RIGHT TO APPEAL.** Any individual adversely affected by an AI-assisted decision has the right to:
   (1) Request human review of the decision;
   (2) Receive an explanation of the AI's role;
   (3) Present additional information;
   (4) Receive a decision from a human decision-maker.

**(b) TIMELINE.** Appeals shall be decided within:
   (1) 48 hours for urgent matters (liberty, safety);
   (2) 14 days for significant matters (employment, benefits);
   (3) 30 days for other matters.

**(c) ACCESSIBILITY.** Appeal processes shall be:
   (1) Free of charge;
   (2) Available in multiple languages;
   (3) Accessible to persons with disabilities;
   (4) Available through multiple channels (online, phone, in-person).

---

## TITLE VII: ENFORCEMENT

### Section 701. NAICO Enforcement Authority

**(a) INVESTIGATION.** NAICO may investigate potential violations through:
   (1) Subpoena of documents and data;
   (2) Interviews of personnel;
   (3) Technical audits of systems;
   (4) On-site inspections.

**(b) CIVIL PENALTIES.** NAICO may impose civil penalties of:
   (1) Up to $50,000 per day for registration violations;
   (2) Up to $100,000 per day for transparency violations;
   (3) Up to $500,000 per incident for bias violations;
   (4) Up to $1 million per incident for prohibited systems.

**(c) INJUNCTIVE RELIEF.** NAICO may seek injunctive relief in federal court to:
   (1) Halt deployment of non-compliant systems;
   (2) Compel compliance with orders;
   (3) Provide relief to affected individuals.

### Section 702. Private Right of Action

**(a) RIGHT TO SUE.** Individuals harmed by violations of this Act may bring civil actions for:
   (1) Actual damages;
   (2) Statutory damages of $1,000 to $10,000 per violation;
   (3) Punitive damages for willful violations;
   (4) Injunctive relief;
   (5) Attorneys' fees.

**(b) CLASS ACTIONS.** Class actions are permitted for systemic violations.

### Section 703. Criminal Penalties

**(a) WILLFUL VIOLATIONS.** Willful deployment of prohibited AI systems, or willful obstruction of NAICO investigations, shall be punishable by:
   (1) Fine up to $10 million for corporations;
   (2) Fine up to $250,000 for individuals;
   (3) Imprisonment up to 5 years for individuals.

---

## TITLE VIII: SAFETY AND EXISTENTIAL RISK

### Section 801. AI Safety Requirements

**(a) GENERAL DUTY.** Developers and deployers of high-capability AI systems have a duty to ensure their systems do not pose unreasonable risks to safety.

**(b) FRONTIER AI.** AI systems meeting capability thresholds defined by NAICO shall:
   (1) Undergo pre-deployment safety testing;
   (2) Implement safety controls;
   (3) Report concerning capabilities to NAICO;
   (4) Participate in information sharing.

### Section 802. Emergency Authority

**(a) EMERGENCY POWERS.** Upon finding that an AI system poses imminent threat to public safety, the Director may:
   (1) Order immediate suspension of deployment;
   (2) Require immediate corrective action;
   (3) Access system for investigation.

**(b) DURATION.** Emergency orders shall be effective for 30 days, renewable upon NAICO finding of continued threat.

**(c) REVIEW.** Emergency orders may be challenged in federal court on expedited basis.

---

## TITLE IX: GENERAL PROVISIONS

### Section 901. Preemption

**(a) FLOOR NOT CEILING.** This Act establishes minimum standards; states may enact more protective requirements.

**(b) NO PREEMPTION OF STRONGER STATE LAW.** State laws providing greater protection for individuals are not preempted.

### Section 902. Relationship to Existing Law

**(a) CIVIL RIGHTS LAWS.** This Act supplements and does not limit civil rights laws.

**(b) SECTOR-SPECIFIC REGULATION.** Sector-specific regulators (FDA, FTC, EEOC, etc.) retain authority over AI in their jurisdictions.

### Section 903. Effective Dates

**(a) NAICO ESTABLISHMENT.** NAICO shall be established within 6 months of enactment.

**(b) REGISTRATION.** Registration requirements take effect 12 months after enactment.

**(c) FULL COMPLIANCE.** Full compliance with all requirements is required 24 months after enactment.

### Section 904. Severability

If any provision of this Act is held unconstitutional, the remainder shall continue in effect.

---

*This model legislation is designed for adaptation and introduction. Specific provisions should be updated based on current AI capabilities and political context.*
