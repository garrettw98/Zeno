# H.R. XXXX / S. XXXX
# THE TIERED TRANSPARENCY AI GOVERNANCE ACT OF 2027
## "The Algorithmic Accountability Act"

> **VERSION 2.0 UPDATE**: This Act has been updated to implement a **Tiered Transparency Framework** replacing the original "Total Open Source Mandate." Tier 1 (Public Commons) AI remains fully open source. Tier 2 (Secure Audit) AI for defense/critical infrastructure is "Source Available" only to vetted NAICO auditors. This prevents proliferation of advanced AI weapons while maintaining democratic accountability.

---

# 118th CONGRESS
## 2nd Session

**To require transparency and open source availability of artificial intelligence systems used by Federal agencies for administrative decisions, to establish the AI Policy Council, to create the National AI Repository, to protect citizens from algorithmic discrimination, and for other purposes.**

---

## IN THE HOUSE OF REPRESENTATIVES / IN THE SENATE

Mr./Ms. _____________ introduced the following bill; which was referred to the Committee on Oversight and Accountability, the Committee on Science, Space, and Technology, and the Committee on the Judiciary

---

# A BILL

To require transparency and open source availability of artificial intelligence systems used by Federal agencies for administrative decisions, to establish the AI Policy Council, to create the National AI Repository, to protect citizens from algorithmic discrimination, and for other purposes.

*Be it enacted by the Senate and House of Representatives of the United States of America in Congress assembled,*

---

## SECTION 1. SHORT TITLE; TABLE OF CONTENTS

### (a) SHORT TITLE
This Act may be cited as the "Tiered Transparency AI Governance Act of 2027" or the "Algorithmic Accountability Act."

### (b) TABLE OF CONTENTS

```
TITLE I—FINDINGS, PURPOSES, AND DEFINITIONS
  Sec. 101. Findings
  Sec. 102. Purposes
  Sec. 103. Definitions

TITLE II—OPEN SOURCE REQUIREMENTS FOR GOVERNMENT AI
  Sec. 201. Mandatory Open Source for Federal AI Systems
  Sec. 202. Prohibited Uses of Proprietary AI
  Sec. 203. Exceptions and Waivers
  Sec. 204. Transition Timeline
  Sec. 205. Existing System Inventory

TITLE III—THE NATIONAL AI REPOSITORY
  Sec. 301. Establishment
  Sec. 302. Repository Requirements
  Sec. 303. Public Access
  Sec. 304. Security Standards
  Sec. 305. Version Control and Audit Trails
  Sec. 306. Community Contributions

TITLE IV—THE AI POLICY COUNCIL
  Sec. 401. Establishment
  Sec. 402. Composition
  Sec. 403. Duties and Responsibilities
  Sec. 404. Human-Centered Optimization Targets
  Sec. 405. Rulemaking Authority
  Sec. 406. Reporting Requirements

TITLE V—ALGORITHMIC DECISION-MAKING STANDARDS
  Sec. 501. Scope of Application
  Sec. 502. Required Disclosures
  Sec. 503. Explainability Requirements
  Sec. 504. Bias Testing and Auditing
  Sec. 505. Human Override Requirements
  Sec. 506. Prohibited Algorithmic Practices

TITLE VI—CITIZEN RIGHTS AND PROTECTIONS
  Sec. 601. Right to Explanation
  Sec. 602. Right to Human Review
  Sec. 603. Right to Appeal
  Sec. 604. Right to Data Access
  Sec. 605. Right to Correction
  Sec. 606. Protection from Discrimination
  Sec. 607. Private Right of Action

TITLE VII—FEDERAL AI ADMINISTRATION
  Sec. 701. AI-Eligible Administrative Functions
  Sec. 702. Human-Reserved Functions
  Sec. 703. Escalation Framework
  Sec. 704. Performance Metrics
  Sec. 705. Pilot Programs

TITLE VIII—DATA SOVEREIGNTY AND PRIVACY
  Sec. 801. Citizen Data Ownership
  Sec. 802. Minimal Collection Principle
  Sec. 803. Encryption Requirements
  Sec. 804. Access Logging
  Sec. 805. Data Portability
  Sec. 806. Penalties for Violations

TITLE IX—SECURITY REQUIREMENTS
  Sec. 901. Security Standards
  Sec. 902. Penetration Testing
  Sec. 903. Bug Bounty Program
  Sec. 904. Incident Response
  Sec. 905. Quantum Resistance Planning

TITLE X—ANTI-CORRUPTION PROVISIONS
  Sec. 1001. Procurement Automation
  Sec. 1002. Benefits Administration
  Sec. 1003. Regulatory Enforcement
  Sec. 1004. Anti-Corruption Metrics

TITLE XI—ENFORCEMENT AND PENALTIES
  Sec. 1101. Inspector General Oversight
  Sec. 1102. Civil Penalties
  Sec. 1103. Criminal Penalties
  Sec. 1104. Whistleblower Protections

TITLE XII—IMPLEMENTATION AND AUTHORIZATION
  Sec. 1201. Implementation Timeline
  Sec. 1202. Agency Responsibilities
  Sec. 1203. Technical Assistance
  Sec. 1204. Authorization of Appropriations
```

---

# TITLE I—FINDINGS, PURPOSES, AND DEFINITIONS

## SEC. 101. FINDINGS

Congress finds the following:

(1) **AI IN GOVERNMENT**—Federal agencies are increasingly deploying artificial intelligence systems for administrative decisions affecting millions of Americans, including benefits eligibility, immigration status, tax audits, and criminal justice.

(2) **TRANSPARENCY DEFICIT**—Most AI systems currently used by Federal agencies are proprietary "black boxes" that cannot be publicly audited, creating accountability gaps incompatible with democratic governance.

(3) **CORRUPTION COST**—The United States loses an estimated $300 billion annually to fraud, waste, and abuse in government programs, much of which occurs in administrative functions that could be automated with transparent AI.

(4) **ALGORITHMIC BIAS**—Studies have documented significant bias in AI systems affecting protected groups, including racial bias in criminal justice algorithms, gender bias in hiring systems, and socioeconomic bias in benefits determinations.

(5) **SECURITY THROUGH TRANSPARENCY**—Open source software has proven more secure than proprietary alternatives, as demonstrated by the widespread adoption of Linux, Apache, and other open source projects for critical infrastructure.

(6) **INTERNATIONAL LEADERSHIP**—Other nations, including Albania and Estonia, have begun implementing AI in government administration, and the United States must lead in establishing democratic standards for algorithmic governance.

(7) **CITIZEN TRUST**—Public trust in government is at historic lows, and transparent, explainable AI administration could rebuild confidence by eliminating arbitrary and inconsistent decision-making.

(8) **CONSTITUTIONAL REQUIREMENTS**—Due process protections under the Fifth and Fourteenth Amendments require that citizens understand the basis for government decisions affecting their rights and benefits.

(9) **EFFICIENCY GAINS**—Properly implemented AI administration can dramatically improve government efficiency, reducing wait times from months to minutes while ensuring consistent application of rules.

(10) **HUMAN DIGNITY**—AI systems must be designed to serve human dignity and flourishing, not to maximize efficiency at the expense of individual rights.

## SEC. 102. PURPOSES

The purposes of this Act are—

(1) to require that all artificial intelligence systems used by Federal agencies for administrative decisions be fully transparent and open source;

(2) to establish the AI Policy Council to provide democratic oversight and guidance for government AI systems;

(3) to create the National AI Repository as a public resource for government AI code and documentation;

(4) to protect citizens from algorithmic discrimination and ensure equal treatment under the law;

(5) to guarantee citizens the right to understand, appeal, and obtain human review of algorithmic decisions;

(6) to establish security standards for government AI systems that leverage the benefits of open source development;

(7) to reduce corruption and increase efficiency in government administration through transparent automation;

(8) to preserve human oversight and control over AI systems while enabling beneficial automation;

(9) to establish the United States as the global leader in democratic AI governance; and

(10) to ensure that AI serves human flourishing, not the other way around.

## SEC. 103. DEFINITIONS

In this Act:

### (1) ADMINISTRATIVE DECISION
The term "administrative decision" means any determination by a Federal agency that affects the rights, benefits, privileges, or obligations of an individual, including—
- (A) eligibility determinations for benefits programs;
- (B) licensing and permitting decisions;
- (C) regulatory compliance determinations;
- (D) procurement and contract awards;
- (E) employment decisions;
- (F) immigration decisions;
- (G) tax assessment and audit selections; and
- (H) any other determination that may be appealed under the Administrative Procedure Act.

### (2) ALGORITHMIC DECISION SYSTEM
The term "algorithmic decision system" or "ADS" means any computational process, including those derived from machine learning, statistics, or other data processing techniques, that makes or substantially supports administrative decisions.

### (3) AI POLICY COUNCIL
The term "Council" means the AI Policy Council established under section 401.

### (4) ARTIFICIAL INTELLIGENCE
The term "artificial intelligence" or "AI" means a machine-based system that can, for a given set of human-defined objectives, make predictions, recommendations, or decisions influencing real or virtual environments.

### (5) AUTOMATED DECISION
The term "automated decision" means an administrative decision made by an algorithmic decision system without contemporaneous human involvement.

### (6) BIAS
The term "bias" means systematic and unfair discrimination in algorithmic outputs based on protected characteristics.

### (7) COVERED AGENCY
The term "covered agency" means any Federal agency, department, or instrumentality of the United States.

### (8) EXPLAINABILITY
The term "explainability" means the ability to provide a meaningful explanation of an algorithmic decision in terms that a non-expert can understand.

### (9) HUMAN-CENTERED METRICS
The term "human-centered metrics" means measures of societal outcomes that prioritize human wellbeing over purely economic measures, as defined under section 404.

### (10) MACHINE LEARNING MODEL
The term "machine learning model" means an algorithmic system that improves its performance on tasks through experience or data exposure.

### (11) NATIONAL AI REPOSITORY
The term "Repository" means the National AI Repository established under section 301.

### (12) OPEN SOURCE
The term "open source" means software whose source code is made freely available and may be redistributed and modified, subject to a license approved by the Open Source Initiative or equivalent.

### (13) PROTECTED CHARACTERISTIC
The term "protected characteristic" means race, color, religion, sex (including pregnancy, sexual orientation, and gender identity), national origin, age, disability, genetic information, or veteran status.

### (14) SIGNIFICANT DECISION
The term "significant decision" means an administrative decision that—
- (A) affects benefits, rights, or legal status;
- (B) involves an amount exceeding $10,000;
- (C) could result in deportation, detention, or criminal referral; or
- (D) is designated as significant by the Council.

### (15) SOURCE CODE
The term "source code" means the human-readable form of computer programming instructions.

### (16) TRAINING DATA
The term "training data" means the data used to develop, calibrate, or validate a machine learning model.

---

# TITLE II—OPEN SOURCE REQUIREMENTS FOR GOVERNMENT AI

## SEC. 201. MANDATORY OPEN SOURCE FOR FEDERAL AI SYSTEMS

### (a) TIERED TRANSPARENCY REQUIREMENT (VERSION 2.0)
All algorithmic decision systems deployed by covered agencies shall be classified under the Tiered Transparency Framework and comply with the corresponding transparency requirements:

**(1) TIER 1: PUBLIC COMMONS (DEFAULT)**—The following systems shall be fully open source:
   - (A) Benefits eligibility and administration systems
   - (B) Tax processing and citizen service systems
   - (C) Permit and license processing systems
   - (D) Public records management systems
   - (E) All other civilian administrative systems not classified as Tier 2 or Tier 3

Requirements for Tier 1:
   - Complete source code released under OSI-approved license
   - Published on National AI Repository accessible to public without charge
   - Training data disclosed with appropriate privacy protections
   - Full documentation and explainability provided

**(2) TIER 2: SECURE AUDIT (SOURCE AVAILABLE)**—The following systems shall be "Source Available" to vetted NAICO auditors only:
   - (A) Defense and military planning systems
   - (B) Cybersecurity and critical infrastructure protection
   - (C) Intelligence analysis systems
   - (D) Counter-terrorism systems
   - (E) Systems where full publication would enable adversarial exploitation

Requirements for Tier 2:
   - Source code available to NAICO-certified auditors with security clearance
   - Annual audit reports published (classified portions redacted)
   - Congressional oversight committee access guaranteed
   - NOT published on public internet

**(3) TIER 3: CLASSIFIED (CONGRESSIONAL OVERSIGHT)**—Reserved for highly sensitive operational systems as defined in section 203.

(3) **FULLY DOCUMENTED**—Comprehensive documentation shall accompany the source code, including—
   - (A) a plain-language description of the system's purpose and function;
   - (B) technical specifications for inputs, outputs, and processing logic;
   - (C) training data sources and preprocessing methods;
   - (D) model architecture and hyperparameters;
   - (E) performance metrics and validation results;
   - (F) known limitations and failure modes; and
   - (G) audit history and change log.

(4) **REPRODUCIBLE**—Sufficient information shall be provided to allow independent researchers to reproduce the system's outputs given the same inputs.

### (b) MODEL WEIGHTS AND PARAMETERS
For machine learning models, covered agencies shall publish—
(1) all model weights and learned parameters;
(2) the training methodology and optimization process;
(3) random seeds and initialization values necessary for reproducibility; and
(4) evaluation metrics on standard benchmarks.

### (c) TRAINING DATA
(1) **DISCLOSURE REQUIREMENT**—Covered agencies shall disclose all training data used to develop algorithmic decision systems, subject to subsection (d).

(2) **DATA DOCUMENTATION**—Training data disclosures shall include—
   - (A) data sources and collection methods;
   - (B) preprocessing and cleaning procedures;
   - (C) sampling methodology;
   - (D) known biases or limitations in the data;
   - (E) consent status for personal data; and
   - (F) demographic composition analysis.

### (d) PRIVACY-PRESERVING DISCLOSURE
(1) **PERSONAL DATA**—Training data containing personal information shall be disclosed in anonymized or synthetic form that preserves statistical properties while protecting individual privacy.

(2) **TECHNIQUES**—Acceptable privacy-preserving techniques include—
   - (A) differential privacy;
   - (B) k-anonymity;
   - (C) synthetic data generation; and
   - (D) federated learning documentation.

(3) **CERTIFICATION**—The Council shall certify that privacy-preserving disclosures meet appropriate standards.

### (e) TIMING OF DISCLOSURE
(1) **NEW SYSTEMS**—For algorithmic decision systems deployed after the effective date of this Act, disclosure shall occur before deployment.

(2) **EXISTING SYSTEMS**—For systems deployed before the effective date, disclosure shall occur according to the timeline in section 204.

## SEC. 202. PROHIBITED USES OF PROPRIETARY AI

### (a) PROHIBITION
After the transition period in section 204, no covered agency may—

(1) deploy a proprietary algorithmic decision system for administrative decisions;

(2) enter into a contract for a proprietary algorithmic decision system for administrative decisions;

(3) use outputs from a proprietary algorithmic decision system as the basis for administrative decisions;

(4) accept "trade secret" or "confidential business information" claims as justification for non-disclosure of algorithmic decision systems; or

(5) delegate administrative decisions to any entity using proprietary algorithmic decision systems.

### (b) CONTRACT REQUIREMENTS
All contracts for algorithmic decision systems shall require—
(1) delivery of complete source code under open source license;
(2) delivery of all training data or privacy-preserving equivalents;
(3) delivery of complete documentation meeting the standards of section 201;
(4) assignment of intellectual property rights to the United States; and
(5) prohibition on claims of trade secret protection for any deliverable.

### (c) EXISTING CONTRACTS
(1) **MODIFICATION**—Covered agencies shall negotiate modifications to existing contracts to comply with this section.

(2) **TERMINATION**—If modifications cannot be obtained, covered agencies shall terminate contracts at the earliest practicable date and replace proprietary systems with compliant alternatives.

(3) **NO NEW EXTENSIONS**—No existing contract for a proprietary algorithmic decision system may be extended or renewed.

## SEC. 203. EXCEPTIONS AND WAIVERS

### (a) NATIONAL SECURITY EXCEPTION
(1) **SCOPE**—The requirements of section 201 shall not apply to algorithmic decision systems used for—
   - (A) intelligence activities;
   - (B) cryptographic systems;
   - (C) military command and control;
   - (D) counterintelligence operations; or
   - (E) other activities designated by the President as critical to national security.

(2) **CONGRESSIONAL OVERSIGHT**—Systems excepted under this subsection shall be subject to oversight by the congressional intelligence committees.

(3) **CLASSIFICATION**—Source code and documentation for excepted systems shall be classified at the appropriate level and made available to cleared auditors.

(4) **LIMITATION**—This exception shall not apply to systems making administrative decisions affecting United States persons, except in the context of counterintelligence investigations.

### (b) LAW ENFORCEMENT EXCEPTION
(1) **SCOPE**—Limited exceptions may be granted for law enforcement systems where disclosure would—
   - (A) reveal investigative techniques; or
   - (B) enable circumvention of fraud detection.

(2) **REQUIREMENTS**—Excepted systems must still—
   - (A) be subject to internal audit by the Inspector General;
   - (B) undergo bias testing by cleared external auditors;
   - (C) provide explanations to affected individuals upon request; and
   - (D) be disclosed after operational sensitivity expires.

(3) **DURATION**—Law enforcement exceptions expire after 5 years and may only be renewed upon demonstration of continued need.

### (c) WAIVER PROCESS
(1) **APPLICATION**—A covered agency may apply to the Council for a waiver from specific requirements of this Title.

(2) **CRITERIA**—Waivers may be granted only upon demonstration that—
   - (A) compliance is not technically feasible;
   - (B) the agency has made good faith efforts to comply;
   - (C) alternative transparency measures will be implemented; and
   - (D) the waiver is time-limited with a plan for eventual compliance.

(3) **PUBLIC NOTICE**—All waiver applications and decisions shall be published on the Repository, except for classified portions.

(4) **CONGRESSIONAL NOTIFICATION**—The Council shall notify the appropriate committees of Congress of any waiver granted within 30 days.

## SEC. 204. TRANSITION TIMELINE

### (a) NEW SYSTEMS
Beginning 1 year after the date of enactment of this Act, all new algorithmic decision systems shall comply with this Title before deployment.

### (b) EXISTING SYSTEMS—PHASE 1
Within 2 years after the date of enactment—
(1) all covered agencies shall complete the inventory required under section 205;
(2) agencies shall publish a transition plan for each system; and
(3) high-impact systems (affecting more than 100,000 individuals annually) shall achieve compliance.

### (c) EXISTING SYSTEMS—PHASE 2
Within 3 years after the date of enactment—
(1) medium-impact systems (affecting 10,000 to 100,000 individuals annually) shall achieve compliance; and
(2) agencies shall demonstrate progress on remaining systems.

### (d) EXISTING SYSTEMS—PHASE 3
Within 5 years after the date of enactment—
(1) all remaining systems shall achieve compliance; and
(2) proprietary systems that cannot be made compliant shall be decommissioned.

### (e) EXTENSION
The Council may grant a 1-year extension for specific systems upon demonstration of good faith progress and a credible plan for compliance.

## SEC. 205. EXISTING SYSTEM INVENTORY

### (a) INVENTORY REQUIREMENT
Within 1 year after the date of enactment of this Act, each covered agency shall complete and publish an inventory of all algorithmic decision systems in use.

### (b) INVENTORY CONTENTS
The inventory shall include, for each system—
(1) the name and purpose of the system;
(2) the agency and office responsible for the system;
(3) the number of individuals affected annually;
(4) the types of decisions made or supported by the system;
(5) the date of deployment;
(6) whether the system is proprietary or open source;
(7) the vendor or developer of the system;
(8) known performance metrics;
(9) known bias audit results; and
(10) the planned compliance pathway.

### (c) PUBLIC AVAILABILITY
The inventory shall be published on the Repository and updated quarterly.

### (d) REPORTING
Each covered agency shall report inventory findings to the Council and the appropriate committees of Congress.

---

# TITLE III—THE NATIONAL AI REPOSITORY

## SEC. 301. ESTABLISHMENT

### (a) IN GENERAL
There is established within the General Services Administration the National AI Repository.

### (b) PURPOSE
The Repository shall serve as—
(1) the central platform for publication of government algorithmic decision system source code;
(2) a resource for government agencies developing AI systems;
(3) a platform for public engagement and contribution;
(4) an archive of all versions and changes to government AI systems; and
(5) a model for democratic AI governance worldwide.

### (c) ADMINISTRATION
(1) **DIRECTOR**—The Repository shall be led by a Director appointed by the Administrator of General Services, with the advice and consent of the Senate.

(2) **QUALIFICATIONS**—The Director shall have demonstrated expertise in—
   - (A) software engineering and open source development;
   - (B) artificial intelligence and machine learning;
   - (C) government technology and digital services; and
   - (D) cybersecurity.

(3) **TERM**—The Director shall serve a term of 5 years.

## SEC. 302. REPOSITORY REQUIREMENTS

### (a) TECHNICAL STANDARDS
The Repository shall—
(1) use industry-standard version control systems;
(2) support multiple programming languages and frameworks;
(3) provide continuous integration and testing capabilities;
(4) implement automated security scanning;
(5) support documentation generation and hosting;
(6) enable issue tracking and discussion; and
(7) provide searchable interfaces for code and documentation.

### (b) METADATA STANDARDS
All code published on the Repository shall include standardized metadata indicating—
(1) the responsible agency and contact;
(2) the decision context and scope;
(3) security classification (if any);
(4) license terms;
(5) dependencies and requirements;
(6) deployment status (development, testing, production); and
(7) audit and review history.

### (c) INTEROPERABILITY
The Repository shall be interoperable with—
(1) existing government code repositories;
(2) major commercial platforms (GitHub, GitLab);
(3) academic and research repositories; and
(4) international government AI repositories.

## SEC. 303. PUBLIC ACCESS

### (a) NO BARRIERS
Access to the Repository shall be available to any person without—
(1) charge or fee;
(2) registration (for viewing);
(3) citizenship or residency requirements; or
(4) approval or vetting.

### (b) DOWNLOAD AND USE
Any person may download, use, modify, and redistribute code from the Repository in accordance with the applicable open source license.

### (c) API ACCESS
The Repository shall provide application programming interfaces (APIs) for programmatic access to code and metadata.

### (d) ACCESSIBILITY
The Repository shall comply with Section 508 of the Rehabilitation Act and Web Content Accessibility Guidelines (WCAG) 2.1 Level AA.

## SEC. 304. SECURITY STANDARDS

### (a) SECURITY REQUIREMENTS
The Repository shall implement—
(1) encryption for all data at rest and in transit;
(2) multi-factor authentication for contributors;
(3) code signing and verification;
(4) automated vulnerability scanning;
(5) intrusion detection and prevention systems;
(6) regular security audits by independent assessors; and
(7) incident response procedures.

### (b) SEPARATION OF CONCERNS
(1) **CODE STORAGE**—Source code shall be stored separately from production deployment systems.

(2) **NO SECRETS**—No credentials, API keys, or other secrets shall be stored in the Repository.

(3) **DEPLOYMENT CONTROLS**—Changes to production systems shall require separate authentication and approval processes.

### (c) NATIONAL SECURITY SYSTEMS
Code for systems covered by national security exceptions shall be stored on separate classified infrastructure with appropriate access controls.

## SEC. 305. VERSION CONTROL AND AUDIT TRAILS

### (a) VERSION CONTROL
(1) **REQUIREMENT**—All code on the Repository shall be under version control that records—
   - (A) every change made;
   - (B) the author of each change;
   - (C) the timestamp of each change;
   - (D) the reason for each change; and
   - (E) the approval for each change.

(2) **IMMUTABILITY**—Version history shall be immutable and tamper-evident using cryptographic methods.

### (b) AUDIT TRAILS
(1) **LOGGING**—The Repository shall maintain comprehensive logs of all access and changes.

(2) **RETENTION**—Logs shall be retained for not less than 10 years.

(3) **AVAILABILITY**—Logs shall be available to Inspectors General, the Council, and congressional oversight committees.

### (c) BLOCKCHAIN ANCHORING
(1) **REQUIREMENT**—Critical version control records shall be anchored to a public blockchain for tamper-evidence.

(2) **IMPLEMENTATION**—The Director shall select an appropriate blockchain and implement anchoring within 2 years of the date of enactment.

## SEC. 306. COMMUNITY CONTRIBUTIONS

### (a) PUBLIC CONTRIBUTIONS
The Repository shall accept contributions from the public, including—
(1) bug reports;
(2) security vulnerability reports;
(3) proposed code improvements;
(4) documentation improvements;
(5) testing contributions; and
(6) translations.

### (b) REVIEW PROCESS
(1) **AGENCY REVIEW**—Public contributions shall be reviewed by the responsible agency before incorporation.

(2) **CRITERIA**—Contributions shall be evaluated based on—
   - (A) technical merit;
   - (B) security implications;
   - (C) consistency with policy objectives; and
   - (D) code quality.

(3) **TRANSPARENCY**—Review decisions shall be documented and publicly visible.

### (c) RECOGNITION
The Repository shall recognize significant contributors through—
(1) public acknowledgment;
(2) contribution statistics; and
(3) contributor profiles.

### (d) BUG BOUNTY INTEGRATION
Public contributions identifying security vulnerabilities may qualify for rewards under the bug bounty program established under section 903.

---

# TITLE IV—THE AI POLICY COUNCIL

## SEC. 401. ESTABLISHMENT

### (a) IN GENERAL
There is established an independent agency to be known as the "AI Policy Council."

### (b) STATUS
The Council shall be an independent establishment in the executive branch and shall not be a part of any other agency.

### (c) HEADQUARTERS
The principal office of the Council shall be in the District of Columbia.

## SEC. 402. COMPOSITION

### (a) MEMBERS
The Council shall be composed of 7 members appointed by the President, by and with the advice and consent of the Senate.

### (b) QUALIFICATIONS
(1) **EXPERTISE**—Members shall collectively have expertise in—
   - (A) artificial intelligence and machine learning;
   - (B) public administration and government operations;
   - (C) civil rights and civil liberties;
   - (D) economics and public policy;
   - (E) ethics and philosophy; and
   - (F) cybersecurity.

(2) **DIVERSITY**—In making appointments, the President shall seek diversity of viewpoint, professional background, and demographic characteristics.

(3) **POLITICAL BALANCE**—Not more than 4 members may be of the same political party.

### (c) TERMS
(1) **LENGTH**—Members shall serve for terms of 7 years, staggered so that no more than 2 terms expire in any year.

(2) **REAPPOINTMENT**—Members may be reappointed for one additional term.

(3) **VACANCIES**—Vacancies shall be filled for the remainder of the unexpired term.

### (d) CHAIRPERSON
(1) **DESIGNATION**—The President shall designate one member to serve as Chairperson.

(2) **TERM**—The Chairperson shall serve in that capacity for a term of 4 years or until the expiration of their membership term, whichever occurs first.

### (e) COMPENSATION
(1) **MEMBERS**—Members shall be compensated at a rate equal to the daily equivalent of the annual rate of basic pay for level III of the Executive Schedule.

(2) **CHAIRPERSON**—The Chairperson shall be compensated at a rate equal to level II of the Executive Schedule.

### (f) CONFLICTS OF INTEREST
(1) **FINANCIAL INTERESTS**—Members may not have financial interests in companies that contract with the Federal Government for AI systems.

(2) **COOLING OFF PERIOD**—Members may not accept employment with such companies for 2 years after leaving the Council.

## SEC. 403. DUTIES AND RESPONSIBILITIES

### (a) PRIMARY DUTIES
The Council shall—

(1) **SET STANDARDS**—Establish standards for algorithmic decision systems used by covered agencies, including—
   - (A) performance requirements;
   - (B) bias testing protocols;
   - (C) explainability requirements;
   - (D) security standards; and
   - (E) documentation requirements.

(2) **DEFINE METRICS**—Define human-centered metrics that government AI systems shall optimize, as specified in section 404.

(3) **REVIEW SYSTEMS**—Review and approve algorithmic decision systems before deployment for significant decisions.

(4) **AUDIT**—Conduct or commission regular audits of deployed systems for compliance, bias, and performance.

(5) **INVESTIGATE**—Investigate complaints regarding algorithmic decisions and systemic issues.

(6) **ENFORCE**—Enforce compliance with this Act, including through the penalties specified in Title XI.

(7) **ADVISE**—Advise Congress and the President on AI governance policy.

(8) **COORDINATE**—Coordinate AI governance efforts across covered agencies.

(9) **EDUCATE**—Conduct public education about government AI systems.

(10) **RESEARCH**—Commission research on AI governance, ethics, and impacts.

### (b) ADVISORY FUNCTION
The Council shall advise covered agencies on—
(1) selection and development of AI systems;
(2) compliance with this Act;
(3) best practices for AI deployment;
(4) emerging AI capabilities and risks; and
(5) workforce implications of AI adoption.

### (c) COORDINATION WITH OTHER AGENCIES
The Council shall coordinate with—
(1) the Office of Management and Budget on procurement policy;
(2) the Office of Personnel Management on workforce matters;
(3) the General Services Administration on technology standards;
(4) the National Institute of Standards and Technology on technical standards;
(5) the Department of Justice on civil rights matters; and
(6) agency Inspectors General on oversight matters.

## SEC. 404. HUMAN-CENTERED OPTIMIZATION TARGETS

### (a) ESTABLISHMENT
The Council shall establish human-centered metrics that government AI systems shall optimize, prioritizing human flourishing over purely economic measures.

### (b) REQUIRED METRICS
Human-centered metrics shall include, at minimum—

(1) **HEALTH AND LONGEVITY**—
   - (A) life expectancy;
   - (B) healthy life years;
   - (C) mental health indicators;
   - (D) preventable mortality rates.

(2) **ECONOMIC SECURITY**—
   - (A) poverty rates;
   - (B) median income;
   - (C) food security;
   - (D) housing stability.

(3) **FREEDOM AND AGENCY**—
   - (A) civil liberties indices;
   - (B) mobility and opportunity;
   - (C) entrepreneurship rates;
   - (D) educational attainment.

(4) **ENVIRONMENTAL QUALITY**—
   - (A) air and water quality;
   - (B) exposure to environmental hazards;
   - (C) access to green spaces.

(5) **SOCIAL COHESION**—
   - (A) trust in institutions;
   - (B) civic participation;
   - (C) community engagement;
   - (D) social support networks.

(6) **EQUITY AND INCLUSION**—
   - (A) disparities by protected characteristics;
   - (B) access to services;
   - (C) representation in outcomes.

### (c) WEIGHTING
(1) **PROCESS**—The Council shall establish a process for weighting competing metrics, with opportunity for public input.

(2) **CONSTRAINTS**—No metric shall be weighted at zero, and equity considerations shall be incorporated as constraints, not just targets.

### (d) UPDATES
The Council shall review and update human-centered metrics not less than every 3 years.

## SEC. 405. RULEMAKING AUTHORITY

### (a) IN GENERAL
The Council shall have authority to promulgate rules and regulations necessary to carry out this Act.

### (b) PROCEDURE
Rulemaking shall follow the procedures of the Administrative Procedure Act (5 U.S.C. 553).

### (c) PUBLIC PARTICIPATION
The Council shall provide meaningful opportunity for public participation in rulemaking, including—
(1) advance notice of proposed rulemaking;
(2) public comment periods of not less than 60 days;
(3) public hearings on significant rules; and
(4) explanation of responses to comments.

## SEC. 406. REPORTING REQUIREMENTS

### (a) ANNUAL REPORT
Not later than March 1 of each year, the Council shall submit to Congress and publish a report including—
(1) an inventory of all algorithmic decision systems in use by covered agencies;
(2) compliance status of each agency;
(3) audit findings and enforcement actions;
(4) citizen complaints received and resolved;
(5) bias testing results aggregate;
(6) human-centered metric outcomes;
(7) research findings and recommendations; and
(8) legislative recommendations.

### (b) SIGNIFICANT INCIDENT REPORTS
The Council shall report to Congress within 30 days of any significant incident, including—
(1) system failures affecting more than 1,000 individuals;
(2) discovery of significant bias;
(3) security breaches; or
(4) substantial negative outcomes.

---

# TITLE V—ALGORITHMIC DECISION-MAKING STANDARDS

## SEC. 501. SCOPE OF APPLICATION

### (a) COVERED DECISIONS
This Title applies to any administrative decision made or substantially supported by an algorithmic decision system.

### (b) SUBSTANTIAL SUPPORT
An algorithmic decision system "substantially supports" a decision if—
(1) the system's output is the primary basis for the decision;
(2) human reviewers override the system's recommendation less than 10 percent of the time; or
(3) the system narrows options to the extent that meaningful human discretion is eliminated.

### (c) EXCLUSIONS
This Title does not apply to—
(1) purely ministerial functions (document routing, scheduling);
(2) internal management decisions not affecting the public;
(3) research and analysis that does not directly determine outcomes; or
(4) systems covered by the national security exception.

## SEC. 502. REQUIRED DISCLOSURES

### (a) PRE-DEPLOYMENT DISCLOSURE
Before deploying an algorithmic decision system, a covered agency shall publish—
(1) a plain-language description of the system's purpose and function;
(2) the types of decisions the system will make or support;
(3) the data inputs the system uses;
(4) the factors the system considers and their relative weights;
(5) the anticipated population affected;
(6) bias testing results;
(7) accuracy and error rate metrics; and
(8) the appeal process for affected individuals.

### (b) NOTICE TO AFFECTED INDIVIDUALS
When an algorithmic decision system is used to make or support a decision affecting an individual, the agency shall provide notice that—
(1) an algorithmic system was used;
(2) describes the role of the system in the decision;
(3) identifies the key factors considered;
(4) explains how to obtain a full explanation; and
(5) explains the appeal process.

## SEC. 503. EXPLAINABILITY REQUIREMENTS

### (a) INDIVIDUAL EXPLANATIONS
Upon request, a covered agency shall provide any individual affected by an algorithmic decision with an explanation that—
(1) is in plain language understandable to a non-expert;
(2) identifies the specific factors that influenced the decision;
(3) explains how those factors were weighted;
(4) identifies the data used about the individual;
(5) explains what different inputs would have changed the outcome; and
(6) is provided within 30 days of the request.

### (b) TECHNICAL EXPLANATIONS
In addition to plain-language explanations, covered agencies shall provide technical explanations to—
(1) academic researchers upon request;
(2) oversight bodies; and
(3) legal representatives of affected individuals.

### (c) EXPLANATION METHODS
Acceptable explanation methods include—
(1) feature importance rankings;
(2) counterfactual explanations;
(3) local interpretable model-agnostic explanations (LIME);
(4) Shapley values;
(5) decision tree approximations; and
(6) other methods approved by the Council.

## SEC. 504. BIAS TESTING AND AUDITING

### (a) PRE-DEPLOYMENT TESTING
Before deployment, each algorithmic decision system shall undergo bias testing that—
(1) evaluates outcomes by protected characteristics;
(2) uses appropriate statistical methods for disparate impact analysis;
(3) tests on data representative of the affected population;
(4) identifies any significant disparities; and
(5) documents mitigation measures for any disparities found.

### (b) ONGOING MONITORING
After deployment, covered agencies shall—
(1) continuously monitor outcomes by protected characteristics;
(2) conduct annual bias audits;
(3) investigate any emerging disparities; and
(4) report significant disparities to the Council within 30 days.

### (c) INDEPENDENT AUDITS
(1) **REQUIREMENT**—Each algorithmic decision system shall undergo independent bias audit by an entity not involved in the system's development at least every 2 years.

(2) **AUDITOR QUALIFICATIONS**—The Council shall establish qualifications for independent auditors.

(3) **PUBLICATION**—Audit results shall be published on the Repository.

### (d) STANDARDS
The Council shall establish specific bias testing standards, including—
(1) statistical thresholds for acceptable disparity;
(2) required demographic categories for analysis;
(3) appropriate baseline comparisons; and
(4) documentation requirements.

## SEC. 505. HUMAN OVERRIDE REQUIREMENTS

### (a) OVERRIDE CAPABILITY
Every algorithmic decision system shall include the capability for authorized humans to override any decision.

### (b) OVERRIDE TRACKING
Covered agencies shall track and report—
(1) the number of decisions made by each system;
(2) the number of human overrides;
(3) the reasons for overrides; and
(4) patterns in override decisions.

### (c) MEANINGFUL REVIEW
(1) **REQUIREMENT**—For significant decisions, human review shall be meaningful, not merely pro forma confirmation of algorithmic outputs.

(2) **STANDARDS**—The Council shall establish standards for meaningful human review, including—
   - (A) time allocated for review;
   - (B) information provided to reviewers;
   - (C) authority of reviewers to override; and
   - (D) reviewer training requirements.

### (d) AUTOMATED DECISIONS
Fully automated decisions (without contemporaneous human involvement) may be made only for—
(1) decisions that are favorable to the individual;
(2) decisions that are not significant under section 103(14);
(3) decisions that are readily reversible; or
(4) decisions for which the individual has expressly consented to automated processing.

## SEC. 506. PROHIBITED ALGORITHMIC PRACTICES

### (a) PROHIBITED PRACTICES
No covered agency may use an algorithmic decision system that—

(1) **USES PROHIBITED FACTORS**—Makes decisions based on race, religion, national origin, sex, sexual orientation, gender identity, or disability except where such factors are relevant to a legitimate eligibility criterion established by law.

(2) **INFERS PROHIBITED FACTORS**—Uses proxy variables that effectively incorporate prohibited factors.

(3) **LACKS EXPLAINABILITY**—Cannot provide explanations meeting the requirements of section 503.

(4) **PERPETUATES HISTORICAL BIAS**—Is trained on data reflecting historical discrimination without appropriate correction.

(5) **USES SOCIAL MEDIA SURVEILLANCE**—Bases decisions on social media activity, except where directly relevant to the specific program and disclosed to applicants.

(6) **USES FACIAL RECOGNITION FOR ADVERSE DECISIONS**—Uses facial recognition technology as the sole or primary basis for adverse decisions, except for identity verification.

(7) **USES EMOTION RECOGNITION**—Purports to detect or analyze emotions, mental states, or intent.

(8) **MANIPULATES BEHAVIOR**—Is designed to manipulate individual behavior against the individual's interests.

(9) **CREATES FEEDBACK LOOPS**—Creates or reinforces harmful feedback loops without correction mechanisms.

(10) **LACKS HUMAN OVERSIGHT**—Operates without the human oversight capabilities required by section 505.

### (b) PENALTIES
Use of prohibited algorithmic practices shall subject the agency to penalties under Title XI and may give rise to private causes of action under section 607.

---

# TITLE VI—CITIZEN RIGHTS AND PROTECTIONS

## SEC. 601. RIGHT TO EXPLANATION

### (a) IN GENERAL
Every individual affected by an algorithmic decision has the right to receive an explanation of that decision.

### (b) REQUEST PROCESS
(1) **SUBMISSION**—Requests may be submitted in writing, electronically, or orally.

(2) **TIMELINE**—Covered agencies shall provide explanations within 30 days of request.

(3) **NO CHARGE**—No fee may be charged for explanations.

### (c) EXPLANATION CONTENT
Explanations shall meet the requirements of section 503.

## SEC. 602. RIGHT TO HUMAN REVIEW

### (a) IN GENERAL
Every individual affected by an automated decision has the right to request human review of that decision.

### (b) REQUEST PROCESS
(1) **SUBMISSION**—Requests may be submitted in writing, electronically, or orally.

(2) **TIMELINE**—Human review shall be completed within 45 days of request.

(3) **NO CHARGE**—No fee may be charged for human review.

### (c) REVIEW STANDARDS
Human review shall—
(1) be conducted by a qualified human decision-maker;
(2) include full consideration of the individual's circumstances;
(3) not be limited to confirming the algorithmic output;
(4) result in a written determination with explanation; and
(5) provide information about further appeal rights.

## SEC. 603. RIGHT TO APPEAL

### (a) ADMINISTRATIVE APPEAL
Every individual may appeal any algorithmic decision through the agency's administrative appeal process.

### (b) COUNCIL REVIEW
If an individual believes an algorithmic system is systematically flawed, they may file a complaint with the Council.

### (c) JUDICIAL REVIEW
Nothing in this Act shall limit the right to seek judicial review of agency actions under the Administrative Procedure Act or other applicable law.

### (d) CONTINUATION OF BENEFITS
During the pendency of an appeal of a decision to terminate or reduce benefits, such benefits shall continue unless the agency demonstrates fraud or imminent harm.

## SEC. 604. RIGHT TO DATA ACCESS

### (a) IN GENERAL
Every individual has the right to access all data held by covered agencies about that individual, including—
(1) data used in algorithmic decisions;
(2) outputs of algorithmic analysis;
(3) records of algorithmic decisions; and
(4) any scores, ratings, or classifications generated.

### (b) REQUEST PROCESS
(1) **SUBMISSION**—Requests may be submitted in writing or electronically.

(2) **TIMELINE**—Covered agencies shall provide data within 30 days of request.

(3) **FORMAT**—Data shall be provided in a structured, commonly used, machine-readable format.

(4) **NO CHARGE**—No fee may be charged for the first request in any 12-month period.

## SEC. 605. RIGHT TO CORRECTION

### (a) IN GENERAL
Every individual has the right to request correction of inaccurate data held by covered agencies.

### (b) PROCESS
(1) **SUBMISSION**—Correction requests may be submitted in writing or electronically with supporting documentation.

(2) **INVESTIGATION**—Covered agencies shall investigate correction requests within 30 days.

(3) **CORRECTION**—If data is determined to be inaccurate, it shall be corrected within 15 days.

(4) **PROPAGATION**—Corrections shall be propagated to all systems using the data and to any entities to which the data was previously disclosed.

### (c) RECONSIDERATION
If data correction affects a prior algorithmic decision, the individual may request reconsideration of that decision.

## SEC. 606. PROTECTION FROM DISCRIMINATION

### (a) PROHIBITION
No individual may be subjected to algorithmic discrimination by a covered agency on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, genetic information, or veteran status.

### (b) DEFINITION
Algorithmic discrimination occurs when an algorithmic decision system produces outcomes that disproportionately and unjustifiably burden members of protected groups.

### (c) AFFIRMATIVE OBLIGATION
Covered agencies have an affirmative obligation to ensure their algorithmic decision systems do not discriminate.

## SEC. 607. PRIVATE RIGHT OF ACTION

### (a) IN GENERAL
Any individual harmed by a violation of this Title may bring a civil action against the covered agency in United States district court.

### (b) REMEDIES
A court may award—
(1) declaratory and injunctive relief;
(2) compensatory damages;
(3) reasonable attorney's fees and costs; and
(4) such other relief as the court determines appropriate.

### (c) STATUTE OF LIMITATIONS
Actions under this section must be brought within 3 years of the alleged violation or 1 year of discovery, whichever is later.

### (d) EXHAUSTION
An individual need not exhaust administrative remedies before bringing an action under this section, except that the individual must have requested an explanation under section 601 at least 30 days before filing suit.

---

# TITLE VII—FEDERAL AI ADMINISTRATION

## SEC. 701. AI-ELIGIBLE ADMINISTRATIVE FUNCTIONS

### (a) ELIGIBLE FUNCTIONS
The following administrative functions are eligible for AI administration, subject to the requirements of this Act—

(1) **TAX ADMINISTRATION**—
   - (A) return processing;
   - (B) refund calculation and issuance;
   - (C) audit selection (subject to human review);
   - (D) compliance monitoring;
   - (E) taxpayer assistance.

(2) **BENEFITS ADMINISTRATION**—
   - (A) eligibility determination for means-tested programs;
   - (B) benefit calculation;
   - (C) enrollment processing;
   - (D) change reporting;
   - (E) fraud detection (subject to human review before adverse action).

(3) **LICENSING AND PERMITTING**—
   - (A) application processing;
   - (B) completeness review;
   - (C) standard compliance verification;
   - (D) fee calculation;
   - (E) issuance of routine licenses and permits.

(4) **PROCUREMENT**—
   - (A) bid evaluation against specified criteria;
   - (B) compliance verification;
   - (C) best value determination;
   - (D) contract monitoring;
   - (E) performance evaluation.

(5) **REGULATORY COMPLIANCE**—
   - (A) monitoring and reporting;
   - (B) initial violation detection;
   - (C) penalty calculation per schedule;
   - (D) compliance assistance.

(6) **RECORDS MANAGEMENT**—
   - (A) FOIA request processing;
   - (B) document classification;
   - (C) redaction per established guidelines;
   - (D) response generation.

### (b) PHASED IMPLEMENTATION
The Council shall establish a phased implementation schedule for AI administration of eligible functions, prioritizing functions with—
(1) high volume of routine decisions;
(2) clear legal standards;
(3) limited discretion; and
(4) significant corruption vulnerability.

## SEC. 702. HUMAN-RESERVED FUNCTIONS

### (a) PROHIBITED AUTOMATION
The following functions may not be fully automated and must retain meaningful human decision-making authority—

(1) **CONSTITUTIONAL FUNCTIONS**—
   - (A) legislation;
   - (B) judicial decisions;
   - (C) executive orders and proclamations;
   - (D) constitutional interpretation.

(2) **LIBERTY INTERESTS**—
   - (A) criminal prosecution decisions;
   - (B) detention decisions;
   - (C) deportation orders;
   - (D) parole and clemency.

(3) **POLICY DECISIONS**—
   - (A) regulatory rulemaking;
   - (B) policy development;
   - (C) resource allocation between programs;
   - (D) priority setting.

(4) **NOVEL SITUATIONS**—
   - (A) cases not covered by existing rules;
   - (B) cases requiring interpretation of ambiguous standards;
   - (C) cases with significant public interest.

### (b) HUMAN JUDGMENT PRESERVATION
Even for AI-eligible functions, agencies shall preserve human judgment for—
(1) appeals of automated decisions;
(2) cases with unusual circumstances;
(3) cases involving potential hardship; and
(4) any case where an individual requests human review.

## SEC. 703. ESCALATION FRAMEWORK

### (a) MANDATORY ESCALATION
Algorithmic decision systems shall automatically escalate to human decision-makers when—

(1) **CONFIDENCE THRESHOLD**—The system's confidence in its recommendation falls below a threshold established by the Council.

(2) **NOVELTY DETECTION**—The case involves factors not well-represented in training data.

(3) **POTENTIAL RIGHTS VIOLATION**—The decision could violate constitutional or statutory rights.

(4) **SIGNIFICANT IMPACT**—The decision involves amounts exceeding thresholds established by the Council.

(5) **PATTERN DETECTION**—The system detects an unusual pattern suggesting potential error.

(6) **INDIVIDUAL REQUEST**—The individual requests human review.

(7) **ADVERSE ACTION**—For significant adverse actions (benefit termination, license revocation, deportation).

### (b) ESCALATION TRACKING
Covered agencies shall track and report escalation statistics, including—
(1) number and percentage of escalated cases;
(2) reasons for escalation;
(3) outcomes of escalated cases; and
(4) time to resolution for escalated cases.

### (c) TRAINING
Human decision-makers who receive escalated cases shall receive training in—
(1) the algorithmic system and its limitations;
(2) the reasons for escalation;
(3) independent evaluation of the case; and
(4) override authority and documentation.

## SEC. 704. PERFORMANCE METRICS

### (a) REQUIRED METRICS
Covered agencies shall track and report the following metrics for each algorithmic decision system—

(1) **VOLUME**—
   - (A) total decisions;
   - (B) automated decisions;
   - (C) escalated decisions;
   - (D) overridden decisions.

(2) **ACCURACY**—
   - (A) error rates by type;
   - (B) appeal rates;
   - (C) reversal rates on appeal.

(3) **EQUITY**—
   - (A) outcomes by protected characteristics;
   - (B) disparity indices;
   - (C) accessibility metrics.

(4) **EFFICIENCY**—
   - (A) processing time;
   - (B) cost per decision;
   - (C) backlog trends.

(5) **CITIZEN EXPERIENCE**—
   - (A) satisfaction surveys;
   - (B) complaint rates;
   - (C) explanation request rates.

### (b) DASHBOARDS
Covered agencies shall publish real-time or near-real-time dashboards displaying performance metrics.

### (c) BENCHMARKS
The Council shall establish performance benchmarks and may require corrective action for systems failing to meet benchmarks.

## SEC. 705. PILOT PROGRAMS

### (a) AUTHORIZATION
Covered agencies may conduct pilot programs to test AI administration of eligible functions.

### (b) REQUIREMENTS
Pilot programs shall—
(1) be approved by the Council;
(2) include rigorous evaluation components;
(3) provide enhanced human oversight;
(4) limit scope and duration;
(5) include safeguards against harm; and
(6) provide for termination if problems emerge.

### (c) REPORTING
Pilot program results shall be reported to the Council and Congress and published on the Repository.

---

# TITLE VIII—DATA SOVEREIGNTY AND PRIVACY

## SEC. 801. CITIZEN DATA OWNERSHIP

### (a) DECLARATION
Data about United States citizens held by covered agencies belongs to those citizens. Covered agencies hold such data as custodians, not owners.

### (b) IMPLICATIONS
This ownership principle means—
(1) citizens have the right to access their data;
(2) citizens have the right to correct inaccurate data;
(3) citizens have the right to request deletion of data no longer needed;
(4) agencies must protect data with the care due to property of another; and
(5) agencies must account for their use of citizen data.

## SEC. 802. MINIMAL COLLECTION PRINCIPLE

### (a) REQUIREMENT
Covered agencies shall collect only the data necessary for specific, documented purposes.

### (b) PROHIBITION ON BULK COLLECTION
Covered agencies may not engage in bulk collection of citizen data without specific statutory authorization.

### (c) DATA MINIMIZATION
When less data would serve the purpose, agencies shall use less data.

### (d) RETENTION LIMITS
Data shall be retained only as long as necessary and shall be deleted when no longer needed.

## SEC. 803. ENCRYPTION REQUIREMENTS

### (a) DATA AT REST
All personal data held by covered agencies shall be encrypted at rest using standards approved by the National Institute of Standards and Technology.

### (b) DATA IN TRANSIT
All transfers of personal data shall be encrypted in transit.

### (c) KEY MANAGEMENT
Encryption key management shall follow NIST guidelines.

### (d) QUANTUM RESISTANCE
Within 5 years of the date of enactment, covered agencies shall transition to quantum-resistant encryption for personal data.

## SEC. 804. ACCESS LOGGING

### (a) REQUIREMENT
All access to citizen data shall be logged, including—
(1) the identity of the accessor;
(2) the time of access;
(3) the data accessed;
(4) the purpose of access; and
(5) the authorization for access.

### (b) IMMUTABILITY
Access logs shall be immutable and tamper-evident.

### (c) CITIZEN ACCESS
Citizens shall have the right to view logs of access to their data.

### (d) RETENTION
Access logs shall be retained for not less than 7 years.

## SEC. 805. DATA PORTABILITY

### (a) RIGHT TO EXPORT
Citizens have the right to export all data held about them by covered agencies in a structured, commonly used, machine-readable format.

### (b) INTEROPERABILITY
The Council shall establish standards for data portability to enable interoperability between government systems and with private systems as appropriate.

## SEC. 806. PENALTIES FOR VIOLATIONS

### (a) CIVIL PENALTIES
Violations of this Title may subject agencies and individuals to civil penalties under Title XI.

### (b) CRIMINAL PENALTIES
Willful violations involving unauthorized access to or disclosure of citizen data may subject individuals to criminal penalties under Title XI.

---

# TITLE IX—SECURITY REQUIREMENTS

## SEC. 901. SECURITY STANDARDS

### (a) BASELINE STANDARDS
All algorithmic decision systems shall comply with—
(1) NIST Cybersecurity Framework;
(2) FISMA requirements;
(3) FedRAMP standards for cloud systems; and
(4) additional standards established by the Council.

### (b) AI-SPECIFIC STANDARDS
The Council, in consultation with NIST, shall establish AI-specific security standards addressing—
(1) model integrity and tampering;
(2) training data poisoning;
(3) adversarial attacks;
(4) model extraction;
(5) inference attacks on training data; and
(6) prompt injection and manipulation.

### (c) SUPPLY CHAIN SECURITY
Covered agencies shall implement supply chain security measures for AI systems, including—
(1) verification of component provenance;
(2) vulnerability tracking for dependencies;
(3) assessment of foreign component risks; and
(4) software bill of materials.

## SEC. 902. PENETRATION TESTING

### (a) REQUIREMENT
Each algorithmic decision system shall undergo penetration testing—
(1) before deployment;
(2) annually after deployment;
(3) after significant changes; and
(4) as directed by the Council based on emerging threats.

### (b) INDEPENDENCE
Penetration testing shall be conducted by entities independent of the system's developers.

### (c) REMEDIATION
Vulnerabilities identified through penetration testing shall be remediated before deployment or within 30 days for deployed systems.

## SEC. 903. BUG BOUNTY PROGRAM

### (a) ESTABLISHMENT
There is established a Federal AI Bug Bounty Program.

### (b) SCOPE
The program shall offer rewards for—
(1) security vulnerabilities in government AI systems;
(2) bias discoveries;
(3) functional errors; and
(4) documentation deficiencies.

### (c) REWARDS
(1) **AMOUNTS**—Rewards shall range from $500 to $100,000 based on severity.

(2) **FUNDING**—There is authorized to be appropriated $10,000,000 annually for the bug bounty program.

### (d) LEGAL PROTECTION
Good-faith security research under the bug bounty program shall not give rise to civil or criminal liability.

## SEC. 904. INCIDENT RESPONSE

### (a) INCIDENT RESPONSE PLANS
Each covered agency shall maintain an incident response plan for AI system security incidents.

### (b) NOTIFICATION
(1) **COUNCIL NOTIFICATION**—Security incidents shall be reported to the Council within 24 hours.

(2) **AFFECTED INDIVIDUALS**—If an incident affects citizen data, affected individuals shall be notified within 72 hours.

(3) **CONGRESSIONAL NOTIFICATION**—Significant incidents shall be reported to Congress within 7 days.

### (c) POST-INCIDENT REVIEW
Following any significant incident, the agency shall conduct a post-incident review and publish lessons learned.

## SEC. 905. QUANTUM RESISTANCE PLANNING

### (a) ASSESSMENT
Within 1 year of the date of enactment, the Council shall assess the quantum computing threat to government AI systems.

### (b) TRANSITION PLAN
Within 2 years of the date of enactment, the Council shall publish a transition plan for quantum-resistant cryptography.

### (c) IMPLEMENTATION
Critical systems shall implement quantum-resistant cryptography within 5 years of the date of enactment.

---

# TITLE X—ANTI-CORRUPTION PROVISIONS

## SEC. 1001. PROCUREMENT AUTOMATION

### (a) AI PROCUREMENT SYSTEM
The General Services Administration shall develop and deploy an open-source AI procurement system that—
(1) evaluates bids against published criteria;
(2) ensures blind evaluation (vendor identity revealed only after selection);
(3) documents all evaluation factors and weights;
(4) produces audit trail for every decision;
(5) flags potential conflicts of interest; and
(6) monitors contract execution for anomalies.

### (b) MANDATORY USE
Beginning 3 years after the date of enactment, all Federal procurements exceeding $250,000 shall use the AI procurement system.

### (c) HUMAN OVERSIGHT
(1) **REVIEW THRESHOLD**—Procurements exceeding $10,000,000 shall receive human review before award.

(2) **APPEALS**—Unsuccessful bidders may appeal to the Government Accountability Office, which shall have access to all AI decision factors.

## SEC. 1002. BENEFITS ADMINISTRATION

### (a) AI BENEFITS SYSTEM
The Social Security Administration, in coordination with other benefits-administering agencies, shall develop and deploy open-source AI systems for benefits administration that—
(1) determine eligibility based on published criteria;
(2) process applications consistently;
(3) detect potential fraud without bias;
(4) ensure timely processing; and
(5) provide clear explanations to applicants.

### (b) SAFEGUARDS
Benefits AI systems shall—
(1) favor applicants in ambiguous cases;
(2) escalate to humans before adverse determinations;
(3) provide clear appeal pathways; and
(4) not terminate benefits without human review.

## SEC. 1003. REGULATORY ENFORCEMENT

### (a) AI REGULATORY SYSTEM
Regulatory agencies shall develop open-source AI systems for compliance monitoring that—
(1) monitor regulated entities consistently;
(2) detect potential violations;
(3) calculate penalties per published schedules;
(4) document enforcement rationale; and
(5) ensure equal treatment regardless of entity size or influence.

### (b) LIMITATIONS
(1) **HUMAN DECISION FOR SIGNIFICANT PENALTIES**—Penalties exceeding $100,000 shall require human approval.

(2) **NO SELECTIVE ENFORCEMENT**—AI systems shall not enable selective enforcement based on political or other improper factors.

## SEC. 1004. ANTI-CORRUPTION METRICS

### (a) REQUIRED METRICS
Covered agencies shall track and report metrics indicating corruption risk, including—
(1) variance in processing times;
(2) variance in outcomes;
(3) override patterns;
(4) complaint patterns;
(5) relationship of outcomes to applicant characteristics; and
(6) deviation from algorithmic recommendations.

### (b) BENCHMARKS
The Council shall establish benchmarks for corruption risk metrics and investigate anomalies.

---

# TITLE XI—ENFORCEMENT AND PENALTIES

## SEC. 1101. INSPECTOR GENERAL OVERSIGHT

### (a) AUTHORITY
Inspectors General of covered agencies shall have authority to—
(1) audit algorithmic decision systems;
(2) investigate complaints;
(3) access source code, training data, and logs;
(4) require testimony from agency personnel; and
(5) recommend corrective action.

### (b) ANNUAL AUDITS
Each Inspector General shall conduct annual audits of the agency's compliance with this Act.

### (c) REPORTING
Inspectors General shall report findings to the Council and Congress.

## SEC. 1102. CIVIL PENALTIES

### (a) AGENCY VIOLATIONS
A covered agency that violates this Act may be subject to—
(1) remedial orders from the Council;
(2) budget penalties imposed by Congress; and
(3) requirements to suspend use of non-compliant systems.

### (b) INDIVIDUAL VIOLATIONS
An officer or employee of a covered agency who willfully violates this Act may be subject to—
(1) civil penalty of up to $100,000 per violation;
(2) suspension or removal from position; and
(3) debarment from Federal service.

### (c) CONTRACTOR VIOLATIONS
A contractor that provides a system that violates this Act may be subject to—
(1) contract termination;
(2) requirement to cure violations at contractor expense;
(3) civil penalties of up to $1,000,000; and
(4) debarment from Federal contracting.

## SEC. 1103. CRIMINAL PENALTIES

### (a) UNAUTHORIZED ACCESS
Any person who knowingly accesses an algorithmic decision system or citizen data without authorization shall be subject to—
(1) fine of up to $250,000;
(2) imprisonment for up to 5 years; or
(3) both.

### (b) DATA DISCLOSURE
Any officer or employee who knowingly discloses citizen data in violation of this Act shall be subject to—
(1) fine of up to $250,000;
(2) imprisonment for up to 5 years; or
(3) both.

### (c) MANIPULATION
Any person who knowingly manipulates an algorithmic decision system to produce biased or fraudulent outcomes shall be subject to—
(1) fine of up to $500,000;
(2) imprisonment for up to 10 years; or
(3) both.

## SEC. 1104. WHISTLEBLOWER PROTECTIONS

### (a) PROTECTION
No covered agency may take adverse action against an employee who—
(1) discloses information about violations of this Act;
(2) refuses to participate in violations; or
(3) cooperates with investigations.

### (b) REMEDIES
An employee subject to retaliation may obtain—
(1) reinstatement;
(2) back pay;
(3) compensatory damages;
(4) attorney's fees; and
(5) such other relief as appropriate.

### (c) PROCEDURE
Claims under this section shall be filed with the Merit Systems Protection Board or, for contractor employees, the Department of Labor.

---

# TITLE XII—IMPLEMENTATION AND AUTHORIZATION

## SEC. 1201. IMPLEMENTATION TIMELINE

### (a) YEAR 1
Within 1 year of the date of enactment—
(1) the Council shall be established and confirmed;
(2) the Repository shall be operational;
(3) initial standards shall be published;
(4) the system inventory required under section 205 shall begin.

### (b) YEAR 2
Within 2 years of the date of enactment—
(1) the system inventory shall be complete;
(2) transition plans for existing systems shall be published;
(3) new system requirements shall take effect;
(4) pilot programs shall commence.

### (c) YEAR 3-5
Within 3 to 5 years of the date of enactment—
(1) high and medium-impact systems shall achieve compliance;
(2) AI administration pilots shall expand;
(3) citizen rights protections shall be fully operational.

### (d) YEAR 5+
Beginning 5 years after the date of enactment—
(1) all systems shall achieve compliance;
(2) full AI administration of eligible functions shall be available;
(3) continuous improvement based on experience.

## SEC. 1202. AGENCY RESPONSIBILITIES

### (a) CHIEF AI OFFICERS
Each covered agency shall designate a Chief AI Officer responsible for—
(1) compliance with this Act;
(2) coordination with the Council;
(3) oversight of agency AI systems; and
(4) reporting.

### (b) TRAINING
Each covered agency shall provide training on this Act to—
(1) personnel who develop or deploy AI systems;
(2) personnel who make or review algorithmic decisions;
(3) personnel who handle citizen data; and
(4) leadership.

## SEC. 1203. TECHNICAL ASSISTANCE

### (a) GSA ASSISTANCE
The General Services Administration shall provide technical assistance to covered agencies in—
(1) migrating to open source systems;
(2) using the Repository;
(3) implementing security standards; and
(4) developing compliant systems.

### (b) NIST STANDARDS
The National Institute of Standards and Technology shall develop technical standards and guidance for implementation of this Act.

## SEC. 1204. AUTHORIZATION OF APPROPRIATIONS

### (a) AI POLICY COUNCIL
There is authorized to be appropriated to the AI Policy Council—
(1) $50,000,000 for fiscal year 2028;
(2) $75,000,000 for fiscal year 2029;
(3) $100,000,000 for fiscal year 2030; and
(4) such sums as may be necessary for subsequent fiscal years.

### (b) NATIONAL AI REPOSITORY
There is authorized to be appropriated for the National AI Repository—
(1) $100,000,000 for fiscal year 2028;
(2) $150,000,000 for fiscal year 2029; and
(3) such sums as may be necessary for subsequent fiscal years.

### (c) AGENCY IMPLEMENTATION
There is authorized to be appropriated for agency implementation of this Act—
(1) $500,000,000 for fiscal year 2028;
(2) $1,000,000,000 for fiscal year 2029;
(3) $1,500,000,000 for fiscal year 2030; and
(4) such sums as may be necessary for subsequent fiscal years.

### (d) BUG BOUNTY PROGRAM
There is authorized to be appropriated $10,000,000 annually for the bug bounty program established under section 903.

---

## SEVERABILITY

If any provision of this Act, or the application of such provision to any person or circumstance, is held to be unconstitutional, the remainder of this Act, and the application of the provisions of such to any person or circumstance, shall not be affected thereby.

---

**END OF BILL TEXT**
